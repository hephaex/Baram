---
id: 029_0003001690
title: "“챗GPT, 싸게 써 볼까”…데이터센터 대신 ‘엣지 GPU’ 활용"
category: 
publisher: 디지털타임스
author: 
		
			이준기 기자(bongchu@dt.co.kr)
		
	
published_at: 2025-12-28 00:00
crawled_at: 2025-12-28 03:02:15
url: https://n.news.naver.com/mnews/article/029/0003001690
oid: 029
aid: 0003001690
content_hash: ae396a8fc042ad5292822c4e9cab47645b3828709ed4946558eb1fc2ba9dff67
---

# “챗GPT, 싸게 써 볼까”…데이터센터 대신 ‘엣지 GPU’ 활용

**디지털타임스** | 2025-12-28 00:00 | 

---

KAIST, 소비자급 GPU로 추론 인프라 비용 67% 절감엣지 GPU 활용한 AI 서비스 기술 ‘스펙엣지’ 개발 데이터센터 LLM 연산을 엣지로 분산..인프라 비용 싸게

데이터센터의 그래픽처리장치(GPU)를 덜 쓰고, 주변에 있는 저렴한 GPU를 활용해 인공지능(AI) 서비스를 싸게 제공하는 기술이 개발됐다. AI 서비스 시장 진입 장벽을 낮춰 모두를 위한 AI 활용 환경을 조성하는 데 기여할 것으로 기대된다.KAIST는 한동수 전기및전자공학부 교수 연구팀이 저렴한 소비자급 GPU를 활용해 대규모언어모델(LLM) 인프라 비용을 크게 낮출 수 있는 ‘스펙엣지’를 개발했다고 28일 밝혔다.LLM의 대규모 추론은 대부분 고가의 데이터센터 GPU에 의존하고 있어 서비스 운영 비용이 높고, AI 활용 진입 장벽도 컸다.연구팀은 데이터센터 GPU와 개인 PC나 소형 서버 등에 탑재된 엣지 GPU를 LLM 추론 인프라로 활용해 기존 데이터센터 GPU 대비 토큰당 비용을 약 67.6% 줄였다.엣지 GPU에 배치된 소형언어모델이 확률 높은 토큰 시퀀스를 빠르게 생성하면 데이터센터의 대규모 언어모델이 이를 일괄 검증하는 ‘추측적 디코딩’을 적용했다. 이 과정에서 엣지 GPU는 서버 응답을 기다리지 않고 계속 단어를 만들어 LLM 추론 속도와 인프라 효율을 동시에 높일 수 있다.연구팀은 데이터센터 GPU에서만 추측적 디코딩을 수행한 결과, 비용 효율성은 1.91배, 서버 처리량은 2.22배 향상됨을 확인했다.일반적인 인터넷 속도에서 문제 없이 작동했고, 서버는 GPU 유휴 시간 없이 더 많은 요청을 동시에 처리했다.한동수 KAIST 교수는 “데이터센터에 집중돼 있는 LLM 연산을 엣지로 분산시켜 AI 서비스 기반이 되는 인프라 비용은 줄이고, AI 활용 접근성을 높일 수 있는 새로운 가능성을 제시한 연구”라며 “앞으로 스마트폰, 개인용 컴퓨터, 신경망 처리장치(NPU) 등 다양한 엣지 기기로 확장하면 고품질 AI 서비스를 보다 많은 사용자에게 제공할 수 있을 것”이라고 말했다.이 연구결과는 최근 미국 샌디에이고에서 열린 AI 분야 국제 학회인 ‘신경정보처리시스템 학회’(NeurIPS)에서 스포트라이트로 발표됐다.

---

*Crawled at: 2025-12-28 03:02:15*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/029/0003001690)*
