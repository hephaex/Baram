---
id: 001_0015830897
title: "[AI픽] 업스테이지, 솔라 오픈 공개…딥시크 R1 성능 앞섰다"
category: 
publisher: 연합뉴스
author: 
		
			오지은(built@yna.co.kr)
		
	
published_at: 2026-01-06 00:00
crawled_at: 2026-01-06 03:49:20
url: https://n.news.naver.com/mnews/article/001/0015830897
oid: 001
aid: 0015830897
content_hash: a33a57610273e0a995214bbaacae871c175735ff37c8ed4b90b3577e014213ae
---

# [AI픽] 업스테이지, 솔라 오픈 공개…딥시크 R1 성능 앞섰다

**연합뉴스** | 2026-01-06 00:00 | 

---

129개 전문가 결합 MoE 모델, 오픈소스 전면 공개한국어 벤치마크서 딥시크 대비 2배 이상 성과

(서울&#x3D;연합뉴스) 오지은 기자 &#x3D; 업스테이지는 자체 개발한 대형언어모델(LLM) 솔라 오픈을 오픈소스로 공개했다고 6일 밝혔다. 솔라 오픈은 업스테이지가 참여 중은 과학기술정보통신무 독자 인공지능(AI) 파운데이션 모델 프로젝트의 첫 번째 결과물이다. 업스테이지에 따르면 솔라 오픈은 데이터 구축부터 학습까지 자체적으로 수행하는 프롬 스크래치 방식으로 개발됐다. 업스테이지는 솔라 오픈을 글로벌 오픈소스 플랫폼 허깅스페이스에 공개하고 개발 과정을 담은 테크 리포트도 발표했다. 업스테이지는 솔라 오픈이 중국 대표 AI 모델인 딥시크 R1 대비 사이즈는 15%에 불과하지만 한국어(110%), 영어(103%), 일본어(106%)에서 딥시크를 웃도는 성과를 거뒀다고 전했다. 특히 한국어 능력의 경우 주요 벤치마크 결과에서 딥시크 R1보다 2배 이상 성능 격차를 보였고, 오픈AI GPT-OSS-120B-미디움과 비교해도 100% 앞선 성능을 기록했다. 고차원적 지식 영역에서도 솔라 오픈은 딥시크 R1이나 오픈AI GPT-OSS-120B-미디움과 대등한 성능을 보였다. 업스테이지는 20조 규모의 고품질 사전학습 데이터셋을 적용해 이러한 성능을 낼 수 있었다고 자평했다. 업스테이지는 한국어 데이터가 부족한 상황을 극복하고자 다양한 합성 데이터와 특화 데이터를 학습에 활용했다. 업스테이지는 향후 해당 데이터셋 일부를 한국지능정보사회진흥원(NIA)의 AI 허브로 개방해 국내 AI 연구 생태계 활성화를 위한 공공재로 환원할 방침이다. 솔라 오픈은 129개 전문가 모델을 혼합한 MoE 구조를 활용해 실제 연산에는 120억개 매개변수만 활성화하는 효율화 방식을 사용했다. 또 그래픽처리장치(GPU) 최적화로 초당 토큰 처리량을 80% 향상하고 자체 강화학습 프레임워크 스냅PO를 개발해 학습 기간을 50% 단축했다. 김성훈 업스테이지 대표는 &quot;솔라 오픈은 업스테이지가 독자적으로 학습한 모델로 한국의 정서와 언어적 맥락을 깊이 이해하는 가장 한국적이면서도 세계적인 AI다&quot;라고 말했다. built@yna.co.kr

---

*Crawled at: 2026-01-06 03:49:20*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/001/0015830897)*
