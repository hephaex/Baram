---
id: 009_0005616999
title: "업스테이지, ‘솔라 오픈’ 공개···딥시크 R1 성능 넘었다"
category: 
publisher: 매일경제
author: 
		
			안선제 기자(ahn.sunje@mk.co.kr)
		
	
published_at: 2026-01-06 00:00
crawled_at: 2026-01-06 04:02:55
url: https://n.news.naver.com/mnews/article/009/0005616999
oid: 009
aid: 0005616999
content_hash: 6dc1be7249128741fd8a6bc7cdaea0c27e35b8c042faaae77ddd6e4fd912ea72
---

# 업스테이지, ‘솔라 오픈’ 공개···딥시크 R1 성능 넘었다

**매일경제** | 2026-01-06 00:00 | 

---

GPU 최적화로 학습기간 50% 단축120억원 규모 인프라 비용 절감효과

업스테이지가 자체 개발한 거대언어모델(LLM) ‘솔라 오픈 100B(이하 솔라 오픈)’를 오픈소스로 공개했다고 6일 밝혔다.‘솔라 오픈’은 업스테이지가 주관사로 참여 중인 과학기술정보통신부의 ‘독자 AI(인공지능) 파운데이션 모델 프로젝트’의 첫 번째 결과물이다. 데이터 구축부터 학습에 이르는 과정 전반을 독자적으로 수행하는 프롬 스크래치 방식으로 개발했다.업스테이지는 해당 모델을 글로벌 오픈소스 플랫폼 ‘허깅페이스’에 공개하고, 개발 과정과 기술적 세부 내용을 담은 테크 리포트도 함께 발표했다.업스테이지는 솔라 오픈이 중국의 ‘딥시크 R1(딥시크 R1-0528-671B)’ 대비 사이즈는 15%에 불과하지만, 한국어(110%), 영어(103%), 일본어(106%) 등 3개 국어 주요 벤치마크 평가에서 이를 상회하는 성과를 거뒀다고 강조했다.특히 한국어 능력에서 크게 앞섰다. 업스테이지에 따르면 한국 문화 이해도, 한국어 지식 등 주요 한국어 벤치마크 결과 딥시크 R1 대비 2배 이상의 성능 격차를 보였으며, 오픈AI의 유사 규모 모델인 ‘GPT-OSS-120B-미디움’과 비교해서도 100% 높은 성능을 기록했다.수학, 복합 지시 수행, 에이전트 등 고차원적 지식 영역에서도 딥시크 R1이나 오픈AI GPT-OSS-120B-미디움과 대등한 성능을 보였다.업스테이지는 한국어 데이터 부족을 극복하고자 다양한 합성 데이터와 금융·법률·의학 등 분야별 특화 데이터 등을 학습에 활용하고, 다양한 데이터 학습 및 필터링 방법론을 고도화해 솔라 오픈을 개발했다.향후 업스테이지는 해당 데이터셋의 일부를 한국지능정보사회진흥원(NIA)의 ‘AI 허브’를 통해 개방해 국내 AI 연구 생태계 활성화를 위한 공공재로 환원할 방침이다.솔라 오픈은 129개의 전문가 모델을 혼합한 ‘MoE’ 구조를 통해 실제 연산에는 120억 개 매개변수만 활성화하는 방식으로 효율을 극대화했다.또한 그래픽처리장치(GPU) 최적화를 통해 초당 토큰 처리량(TPS)을 약 80% 향상시키고, 자체 강화학습(RL) 프레임워크 ‘스냅PO’를 개발해 학습 기간을 50% 단축했다. 이를 통해 약 120억원에 달하는 GPU 인프라 비용 절감 효과를 거뒀다는 게 회사 설명이다.김성훈 업스테이지 대표는 “솔라 오픈은 업스테이지가 처음부터 독자적으로 학습해낸 모델로, 한국의 정서와 언어적 맥락을 깊이 이해하는 가장 한국적이면서도 세계적인 AI”라며 “이번 솔라 오픈 공개가 한국형 프런티어 AI 시대를 여는 중요한 전환점이 될 것”이라고 말했다.

---

*Crawled at: 2026-01-06 04:02:55*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/009/0005616999)*
