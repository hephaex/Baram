---
id: 025_0003494074
title: "中 딥시크 &#x27;무서운 진화&#x27; 또 일내나…고효율 AI 학습법 발표"
category: 
publisher: 중앙일보
author: 
		
			베이징&#x3D;이도성 특파원 lee.dosung@joongang.co.kr
		
	
published_at: 2026-01-02 00:00
crawled_at: 2026-01-02 07:45:45
url: https://n.news.naver.com/mnews/article/025/0003494074
oid: 025
aid: 0003494074
content_hash: 7613e02553c6cb6592ca394c9918e2e4919c1978abf0b65f8e393c72accb19b3
---

# 中 딥시크 &#x27;무서운 진화&#x27; 또 일내나…고효율 AI 학습법 발표

**중앙일보** | 2026-01-02 00:00 | 

---

중국 인공지능(AI) 기업 딥시크가 효율성을 더 높인 새로운 AI 학습법이 담긴 논문을 발표했다. 지난해 1월 저비용 고효율 AI 모델을 발표했던 딥시크의 차세대 모델 발표가 임박했다는 관측이 나온다.

딥시크는 최근 과학 분야 사전 논문 공개 사이트 ‘아카이브(arXiv)’에 연구진 19명이 참여한 &#x27;매니폴드 제약 초연결(mHC, Manifold-Constrained Hyper-Connections)&#x27;이라는 제목의 논문을 발표했다. 논문엔 기존 고급 AI 모델이 학습하는 과정에서 있었던 불안정성을 해소하는 동시에 최적화로 성능 향상을 유지하는 방법론이 담겼다. 이를 통해 대규모 AI 시스템 구축에 들어가는 비용과 에너지를 줄일 수 있다는 것이다. 각각 30억·90억·270억 개의 매개변수(parameters) 규모 모델을 통해 mHC를 테스트한 결과다. 기존 딥러닝 훈련 방식과 달리 확장성과 효율성에 중점을 뒀다. 연구진은 논문에서 “mHC는 초연결을 더욱 유연하면서도 실용적으로 확장한 방식으로 현재 AI 모델이 가진 한계를 극복해 인프라 설계의 새로운 길을 열 것”이라고 전망했다. ‘초연결’은 고전적 딥러닝 연결 방식에서 한 걸음 더 나아간 형태다.

창업자 량원펑(梁文峰)도 논문에 공동 저자로 이름 올렸다. 그는 자신의 아이디를 이용해 ‘아카이브’에 직접 논문을 제출했다. 홍콩 사우스차이나모닝포스트(SCMP)는 “딥시크의 명성이 높아지는 동안에도 두문불출했던 량원펑이 딥시크 핵심 연구에 여전히 깊이 관여하고 있다는 증거”라고 평했다. 이번 발표는 다음 달쯤 출시할 것으로 예상되는 딥시크 차세대 AI 모델의 신호탄이다. 딥시크는 주요 모델을 출시하기 전 논문을 먼저 공개하는 방식으로 연구 성과를 알려왔다. 딥시크는 지난해 1월 추론 모델 ‘R1’을 발표하면서 전 세계에 ‘딥시크 충격’을 가져왔다. 미국 실리콘밸리 기업들보다 훨씬 적은 비용으로 고성능 추론 모델을 개발했기 때문이다. 특히 미국의 견제로 고성능 반도체를 수급하기 어려운 상황에서도 대형언어 모델 (LLM)을 훈련해 주목받았다. 후속작인 ‘R2’ 모델은 다음 달 춘제(중국 설) 연휴 무렵 공개될 전망이다. 재스민 류 블룸버그 인텔리전스 애널리스트는 “앞으로 출시될 딥시크의 R2 모델은 구글 등 경쟁사가 뛰어난 성과를 내고 있음에도, 다시 한 번 전 세계 AI 시장을 뒤흔들 수 있을 만한 잠재력을 가지고 있다”고 분석했다.

---

*Crawled at: 2026-01-02 07:45:45*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/025/0003494074)*
