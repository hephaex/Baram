---
id: 092_0002404851
title: "업스테이지, &#x27;솔라 오픈&#x27; 공개…&quot;딥시크·GPT 성능 넘어&quot;"
category: 
publisher: 지디넷코리아
author: 
		
			김미정 기자(notyetkim@zdnet.co.kr)
		
	
published_at: 2026-01-06 00:00
crawled_at: 2026-01-06 04:03:44
url: https://n.news.naver.com/mnews/article/092/0002404851
oid: 092
aid: 0002404851
content_hash: 744ed294effe68cc4e4ba391ba66f81eacbfa9bb43022cf7203d887cf9c47ebb
---

# 업스테이지, &#x27;솔라 오픈&#x27; 공개…&quot;딥시크·GPT 성능 넘어&quot;

**지디넷코리아** | 2026-01-06 00:00 | 

---

한국어 벤치마크서 격차 벌려…美 에포크AI &#x27;주목할 만한 AI 모델&#x27; 포함업스테이지가 자체 개발한 언어모델을 오픈소스로 공개해 인공지능(AI) 경쟁력 강화에 나섰다.업스테이지는 거대언어모델(LMM) &#x27;솔라 오픈 100B&#x27;를 글로벌 오픈소스 플랫폼 허깅페이스에 공개했다고 6일 밝혔다. 해당 모델은 과학기술정보통신부의 &#x27;독자 AI 파운데이션 모델 프로젝트&#x27; 첫 결과물이다.솔라 오픈은 중국 딥시크 R1과 오픈AI GPT-OSS-120B&#x27; 등 글로벌 경쟁 모델을 주요 벤치마크에서 앞선 것으로 나타났다. 특히 한국어, 영어, 일본어 등 다국어 평가에서 모델 크기 대비 우수한 성능을 보였다.

솔라 오픈 벤치마크 기록. (사진&#x3D;업스테이지)특히 한국어 성능에서는 격차가 더 뚜렷했다. 한국 문화 이해도, 한국어 지식 벤치마크에서 딥시크 R1 대비 2배 이상 높은 성능을 보였고, 오픈AI 유사 규모 모델보다 앞선 수치를 기록했다.업스테이지는 이같은 성과가 약 20조 토큰 규모의 고품질 사전학습 데이터와 학습 기법 고도화가 뒷받침했다고 밝혔다. 합성 데이터와 금융, 법률, 의료 등 분야별 특화 데이터를 적극 활용해 저자원 언어 한계를 보완했다는 설명이다.솔라 오픈은 129개 전문가 모델을 섞은 혼합전문가(MoE) 구조를 적용해 실제 연산에는 일부 매개변수만 활성화했다. 이를 통해 초당 토큰 처리량을 높이고 학습 기간을 절반으로 줄여 약 120억원 규모 그래픽처리장치(GPU) 인프라 비용을 절감했다.업스테이지는 일부 데이터셋을 한국지능정보사회진흥원의 AI 허브를 통해 공개해 국내 연구 생태계 활성화에도 나선다. 또 컨소시엄 참여 기관과 함께 금융, 법률, 의료, 공공, 교육 등 산업별 AI 전환 확산을 추진한다.솔라 오픈은 미국 비영리 연구기관 에포크AI의 &#x27;주목할 만한 AI 모델&#x27; 목록에도 이름을 올렸다. 스탠퍼드대 인간중심 AI 연구소 보고서에도 활용돼 한국 AI 기술의 국제적 존재감을 높였다.김성훈 업스테이지 대표는 &quot;솔라 오픈은 우리가 처음부터 독자적으로 학습한 모델&quot;이라며 &quot;한국 정서와 언어적 맥락을 깊이 이해하는 가장 한국적이면서도 세계적인 AI&quot;라고 강조했다.

---

*Crawled at: 2026-01-06 04:03:44*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/092/0002404851)*
