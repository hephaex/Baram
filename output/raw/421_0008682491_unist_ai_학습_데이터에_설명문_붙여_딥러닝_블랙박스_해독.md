---
id: 421_0008682491
title: "UNIST &quot;AI 학습 데이터에 설명문 붙여 딥러닝 블랙박스 해독&quot;"
category: 
publisher: 뉴스1
author: 
		
			김민석 기자 (ideaed@news1.kr)
		
	
published_at: 2025-12-28 00:00
crawled_at: 2025-12-28 01:31:46
url: https://n.news.naver.com/mnews/article/421/0008682491
oid: 421
aid: 0008682491
content_hash: 672105e72a7a74687fd02da89ebf04a2092d27e8e2cb4338c6ef94d9e0aa7049
---

# UNIST &quot;AI 학습 데이터에 설명문 붙여 딥러닝 블랙박스 해독&quot;

**뉴스1** | 2025-12-28 00:00 | 

---

김태환 교수팀 &quot;복잡한 의사결정 근거 추적 학습 방법론 제시&quot;&quot;학습 데이터에 자연어 해설 붙여 AI 투명성 향상 기여&quot;

(서울&#x3D;뉴스1) 김민석 기자 &#x3D; UNIST(울산과학기술원) 인공지능대학원 연구진이 학습 데이터를 인간이 이해할 수 있는 설명문으로 변환해 &#x27;AI 블랙박스&#x27;를 해독하는 새로운 방법을 제시했다.최근 세계적으로 인공지능(AI) 모델이 학습 과정에서 어떤 근거로 판단을 내리는지 &#x27;설명 가능한 AI&#x27;(XAI) 연구가 이어지고 있다.김태환 교수 UNIST 인공지능대학원 연구팀은 AI 학습 데이터를 자연어 문장으로 전환해 모델의 의사결정 근거를 추적하는 학습 방법론을 개발했다고 28일 밝혔다.연구팀은 &quot;AI가 스스로 학습 데이터를 설명하도록 해 복잡한 딥러닝의 의사결정 구조를 직접적으로 들여다볼 수 있다&quot;고 설명했다.딥러닝 기반 AI 모델은 높은 정확도를 보이지만, 내부 작동 원리가 불투명해 &#x27;블랙박스&#x27;로 불리고 있다.김 교수팀은 방향을 바꿔 AI 학습의 출발점인 &#x27;데이터&#x27;에 주목했다. 이미지의 특징을 자연어로 풀어 설명한 후 이를 분석해 모델의 의사결정 과정을 역으로 추적하는 방식이다.

구체적으로는 챗GPT 등 대규모 언어모델(LLM)을 활용해 사진 속 사물이 지닌 특징을 여러 문장으로 기술하게 했다. LLM의 환각(hallucination) 문제를 줄이기 위해 위키백과 등 검증된 외부 지식을 참조하도록 설계했다.이후 연구팀은 생성된 수십 개의 설명문 가운데 실제 학습 성능에 도움이 되는 유효 문장을 가려내기 위해 &#x27;텍스트 영향력 점수&#x27;(IFT·Influence Scores for Texts)를 도입했다.텍스트 영향력 점수 지표는 특정 문장을 학습 데이터에서 제외했을 때 AI 모델의 예측 오차가 얼마나 변하는지를 계산해 기여도를 정량화해 문장이 이미지의 의미와 얼마나 일치하는지 등을 &#x27;CLIP 점수&#x27;를 통해 평가한다.김 교수팀은 이 방식으로 도출한 설명문을 학습 데이터로 다시 사용해 성능 검증 실험을 진행했다. 그 결과 이 방식으로 학습한 모델은 기존 대비 더 안정적이고 높은 성능을 보였다.김태환 교수는 &quot;모델이 실제로 의사결정에 활용한 설명이 최종 학습 효과에도 직접적인 영향을 미친다는 점을 입증한 것&quot;이라며 &quot;AI 시스템을 투명하게 이해하고 신뢰성을 높이는 기반이 될 것&quot;이라고 했다.&lt;용어설명&gt;■ XAIXAI(eXplainable AI·설명 가능한 AI)는 인공지능 모델이 어떤 근거와 과정을 통해 특정 예측·판단을 내렸는지를 인간이 이해할 수 있는 형태로 보여주는 기술·방법론을 말한다. 금융(대출 심사)·의료(진단 보조)·사법(재범 예측) 등 고위험 영역에서 왜 이런 결론을 냈는가를 설명해야 할 필요성이 커지면서 XAI 연구가 본격화했다.■ LLMLarge Language Model. 대규모 언어 모델. 자연어 처리(NLP) 작업을 수행할 수 있는 머신 러닝 모델을 말한다. 자연어의 복잡성을 이해할 수 있어 기존 기계 학습 알고리즘보다 정확하다.■ 환각환각(할루시네이션·Hallucination)은 생성형 인공지능(AI)이 실제로 존재하지 않거나 사실이 아닌 정보를 마치 사실인 것처럼 생성하는 현상을 말한다. 대규모언어모델(LLM)이 존재하지 않는 패턴이나 객체를 인식해 부정확한 답변을 내놓는 현상이다.

---

*Crawled at: 2025-12-28 01:31:46*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/421/0008682491)*
