---
id: 138_0002214422
title: "&#x27;AI 초격차&#x27;…엔비디아, HBM4 최초 탑재한 &#x27;베라 루빈&#x27; 출격 [CES 2026]"
category: 
publisher: 디지털데일리
author: 
		
			라스베이거스(미국)&#x3D;김문기 기자(moon@ddaily.co.kr)
		
	
published_at: 2026-01-06 00:00
crawled_at: 2026-01-06 04:03:55
url: https://n.news.naver.com/mnews/article/138/0002214422
oid: 138
aid: 0002214422
content_hash: 860794d71aa36ef673297551c577da61a760f2330c04c66b5a22812266282416
---

# &#x27;AI 초격차&#x27;…엔비디아, HBM4 최초 탑재한 &#x27;베라 루빈&#x27; 출격 [CES 2026]

**디지털데일리** | 2026-01-06 00:00 | 

---

[인더스트리 AI] 랙 하나가 거대한 프로세서… 220조 개 트랜지스터 집약된 &#x27;NVL72&#x27;의 진화

[라스베이거스(미국)&#x3D;디지털데일리 김문기기자] &quot;AI 훈련과 추론을 위한 컴퓨팅 수요가 어느 때보다도 급증하고 있는 가운데, 루빈의 등장은 매우 시의적절하다. 엔비디아는 매년 차세대 AI 슈퍼컴퓨터를 선보이고 있다. 이러한 가운데, 6개 칩에 고도의 공동 설계를 적용한 루빈은 AI의 새로운 지평을 향해 비약적인 발전을 이룰 것이다.&quot;젠슨 황 엔비디아 CEO는 5일(현지시간) 미국 라스베이거스에서 열린 CES 2026를 맞이해 퐁텐블로 호텔 극장에서 컨퍼런스를 개최하고 &#x27;블랙웰(Blackwell)&#x27;의 후속작, 차세대 슈퍼컴퓨팅 플랫폼 &#x27;베라 루빈(Vera Rubin)&#x27;을 공개했다.베라 루빈은 6개의 핵심 칩이 하나의 유기체처럼 결합된 시스템으로 업계 최초로 6세대 HBM(고대역폭메모리)인 &#x27;HBM4&#x27;를 탑재했다.◆ 6개 칩의 결합, &#x27;베라 루빈&#x27;… 추론 성능 5배 퀀텀점프엔비디아가 공개한 &#x27;베라 루빈&#x27;은 단순한 GPU가 아니다. ▲루빈 GPU ▲베라(Vera) CPU ▲NV링크 6 스위치 ▲CX9 슈퍼닉(SuperNIC) ▲블루필드-4 DPU 등 6개의 칩이 완벽하게 통합된 플랫폼이다.핵심인 &#x27;루빈 GPU&#x27;는 3세대 트랜스포머 엔진을 탑재해 50 페타플롭스(Petaflops)의 연산 성능을 낸다. 특히 업계의 이목이 쏠린 것은 메모리다. 루빈은 세계 최초로 HBM4를 탑재하여 메모리 대역폭을 초당 22테라바이트(TB)까지 끌어올렸다.엔비디아는 &quot;루빈 플랫폼은 이전 세대인 블랙웰 대비 학습 성능은 3.5배, 추론 성능은 무려 5배 향상됐다&quot;며 &quot;거대 언어 모델(LLM)을 구동할 때 비용은 7분의 1 수준으로 줄어든다&quot;고 밝혔다.함께 탑재되는 &#x27;베라 CPU&#x27;는 엔비디아가 자체 설계한 &#x27;올림푸스(Olympus)&#x27; 커스텀 ARM 코어 88개를 내장했다. 기존 그레이스(Grace) CPU 대비 데이터 처리 속도를 2배 높여, 초고속 GPU가 쉬지 않고 일할 수 있도록 데이터를 끊임없이 공급한다.◆ &quot;랙 전체가 하나의 거대한 칩&quot;… 유지보수 시간 100분→6분모든 부품은 &#x27;베라 루빈 NVL72&#x27;라는 하나의 랙(Rack)으로 통합된다. 랙 하나에 무려 220조 개의 트랜지스터가 집약되어 있어, 사실상 데이터센터 랙 전체가 하나의 거대한 프로세서처럼 작동한다.주목할 점은 운영 효율성이다. 수십만 대의 GPU가 돌아가는 AI 팩토리(공장)에서 고장은 곧 비용이다. 엔비디아는 케이블을 없앤 모듈형 설계를 도입해 서비스 가능성을 혁신했다.젠슨 황 CEO는 &quot;기존에는 트레이 교체에 100분이 걸렸다면, 루빈 NVL72 시스템은 단 6분이면 가능하다&quot;며 &quot;이는 수천억 원 규모의 AI 인프라를 운영하는 기업들에게 획기적인 비용 절감 효과를 가져다줄 것&quot;이라고 강조했다.◆&#x27;기억상실&#x27; 막는다… 인퍼런스 컨텍스트 메모리 스토리지엔비디아는 이날 하드웨어뿐만 아니라 새로운 스토리지 아키텍처인 &#x27;인퍼런스 컨텍스트 메모리 스토리지(Inference Context Memory Storage)&#x27;도 발표했다.최근의 AI 에이전트는 사용자와 긴 대화를 나누며 &#x27;맥락(Context)&#x27;을 기억해야 한다. 하지만 GPU 메모리는 비싸고 용량이 제한적이라, 대화가 길어지면 과거 기억을 지우거나 다시 계산해야 하는 비효율이 발생했다.엔비디아는 네트워크에 연결된 별도의 고속 스토리지 계층을 만들어 이 &#x27;기억(KV 캐시)&#x27;을 저장하는 방식을 고안했다. 이를 통해 GPU 메모리를 낭비하지 않으면서도 AI가 무한한 대화 맥락을 유지할 수 있게 됐다.엔비디아는 베라 루빈 플랫폼이 이미 팹(Fab)에서 생산되어 검증 단계를 거치고 있으며, 올해 하반기부터 마이크로소프트 애저(Azure), 코어위브(CoreWeave) 등 주요 파트너사를 통해 본격 공급될 예정이라고 밝혔다.CES 2026 특별취재팀 &#x3D; 라스베이거스(미국) 김문기 부장·배태용·옥송이 기자·취재지원 최민지 팀장·고성현 기자

---

*Crawled at: 2026-01-06 04:03:55*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/138/0002214422)*
