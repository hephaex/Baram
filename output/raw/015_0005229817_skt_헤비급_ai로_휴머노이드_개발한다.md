---
id: 015_0005229817
title: "SKT &#x27;헤비급 AI&#x27;로 휴머노이드 개발한다"
category: 
publisher: 한국경제
author: 
		
			이영애 기자 0ae@hankyung.com
		
	
published_at: 2025-12-28 00:00
crawled_at: 2025-12-28 09:45:59
url: https://n.news.naver.com/mnews/article/015/0005229817
oid: 015
aid: 0005229817
content_hash: 9709d59cea9008ca45b48c680e75322d9fe870f405416dff077b19baff34231c
---

# SKT &#x27;헤비급 AI&#x27;로 휴머노이드 개발한다

**한국경제** | 2025-12-28 00:00 | 

---

독자파운데이션모델 곧 공개5190억개 파라미터로 설계가동할 땐 330억개로 경량화SK하이닉스·이노베이션 등그룹 데이터 동원해 고도화SK텔레콤이 30일 국내 최초로 세계 상위권 성능을 갖춘 인공지능(AI) 모델을 공개한다. 파라미터(매개변수) 5000억 개 규모의 초거대 모델 ‘A.X K1’으로 반도체, 2차전지 등 공정을 검증·고도화하는 제조 AI로 확장하겠다는 전략을 내놨다. 글로벌 빅테크 의존도를 낮추고 AI 주권을 확보하려는 움직임이 가시화되고 있다는 평가다.

◇한국형 AI 계속 나온다SK텔레콤은 이번 프로젝트에서 AI 파라미터를 확대하는 데 주력했다. 복잡한 추론이 가능한 산업용 AI를 구현하기 위해서다. A.X K1은 총 5190억 개 파라미터로 학습했다. 실제 추론 시에는 약 330억 개만 활성화되는 구조다. 초거대 모델의 성능을 유지하면서 서비스 단계에서는 경량화를 달성했다. 파라미터 5000억 개 이상 대규모언어모델(LLM)을 보유한 국가는 미국, 중국 등 극소수다. A.X K1은 중국 딥시크 V3.1(약 6850억 개 파라미터) 등 글로벌 주요 모델에 견줄 경쟁력을 갖췄다는 평가를 받는다.초거대 AI는 일정 규모 이상 파라미터를 갖춰야 언어 맥락 이해와 추론 성능이 안정화된다. SK텔레콤 관계자는 “AI 반도체 제조 공정 검증이나 평가처럼 산업 현장에 바로 적용되는 영역에서는 5000억 개 이상 파라미터가 필요하다”고 설명했다. SK텔레콤은 장기적으로 1조 개 파라미터급 모델 구축을 목표로 하고 있다.SK텔레콤은 A.X K1이 한국어 특화 LLM이라는 것을 강점으로 내세웠다. 한국어 모델 가운데 최대 규모로 국내 제조 현장과 대국민 서비스에서 맥락 이해 능력과 활용성이 높다는 설명이다. SK텔레콤 관계자는 “에이닷 서비스에 1년 이상 적용하며 한국어 이해도와 속도, 비용 효율성을 검증받았다”고 말했다. ◇SK하이닉스 등과 현장 실증SK텔레콤은 SK그룹사를 중심으로 A.X K1을 적용하며 실제 현장 검증을 진행할 계획이다. SK하이닉스, SK이노베이션, SK AX, SK브로드밴드 등 20여 개 기업 및 기관과 협력 방안을 논의 중이다. SK하이닉스와 SK이노베이션과는 반도체 및 2차전지 제조 공정에, SK AX와는 ERP(전사적자원관리) 등에 AI를 적용할 수 있다.서비스 확장도 추진 중이다. 업무 생산성 향상을 위한 ‘에이닷 비즈’ 서비스를 새로 제공하고 크래프톤의 게임 AI를 활용한 실시간 캐릭터 대화 및 자율 행동 구현에도 나선다. 나아가 AI 모델을 물리·행동 영역으로 확장하기 위해 휴머노이드(인간형 로봇) 기술 등으로 활용 분야를 넓힌다는 계획이다.국내 AI 생태계 확산을 위해 오픈소스 개방도 추진한다. 주요 개발 커뮤니티와 SK텔레콤 서비스를 통해 오픈소스와 응용 프로그램 인터페이스(API)를 공개하고 국내 기업을 대상으로 AI 에이전트 개발 환경을 제공할 방침이다.이번 모델은 SK텔레콤이 산학 컨소시엄을 주도해 개발했다. SK텔레콤 외 크래프톤, 포티투닷, 리벨리온, 라이너, 셀렉트스타, 서울대, KAIST 등 8개 기업과 기관으로 구성됐다. AI 반도체와 데이터센터, LLM 모델 개발 등 전체 밸류체인을 독자 기술로 구축한 ‘풀스택 소버린 AI’를 개발하겠다는 계획이다.

---

*Crawled at: 2025-12-28 09:45:59*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/015/0005229817)*
