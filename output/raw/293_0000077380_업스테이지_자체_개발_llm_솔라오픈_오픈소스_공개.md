---
id: 293_0000077380
title: "업스테이지, 자체 개발 LLM &#x27;솔라오픈&#x27; 오픈소스 공개"
category: 
publisher: 블로터
author: 
		
			강준혁 기자(jhkang@bloter.net)
		
	
published_at: 2026-01-06 00:00
crawled_at: 2026-01-06 04:02:01
url: https://n.news.naver.com/mnews/article/293/0000077380
oid: 293
aid: 0000077380
content_hash: 5d2850bccabd3bfd89f87a93eaf844c6ac2c86bb751da1298d84803170faf599
---

# 업스테이지, 자체 개발 LLM &#x27;솔라오픈&#x27; 오픈소스 공개

**블로터** | 2026-01-06 00:00 | 

---

업스테이지는 6일 자체 개발한 대규모언어모델(LLM) &#x27;솔라오픈&#x27;을 오픈소스로 공개했다.솔라오픈은 과학기술정보통신부가 추진하는 독자 인공지능 인공지능(AI) 파운데이션 모델 프로젝트의 첫 성과물이다. 업스테이지는 데이터 구축부터 학습까지 자체 수행하는 &#x27;프롬 스크래치&#x27; 방식으로 솔라오픈을 개발했다고 설명했다. 프롬 스크래치는 외부 사전학습 모델 가중치를 가져다 쓰지 않고 데이터 구축부터 사전학습까지 전 과정을 처음부터 자체로 수행해 모델을 만드는 방식이다.업스테이지는 솔라오픈을 글로벌 오픈소스 플랫폼 허깅페이스에 공개했다. 개발 과정을 담은 기술 보고서도 함께 발표했다. 회사에 따르면 솔라오픈은 중국 AI 모델 &#x27;딥시크R1&#x27; 대비 모델 크기가 15% 수준이지만 한국어 110% 영어 103% 일본어 106% 등에서 더 높은 성능을 기록했다. 특히 한국어 벤치마크에서는 딥시크R1보다 2배 이상 우수한 결과를 보였다. 업스테이지에 따르면 오픈AI &#x27;GPT OSS 120B 미디움&#x27;과 비교해도 약 100% 높은 성능을 냈다.고차원 지식 영역에서는 딥시크R1과 오픈AI GPT OSS 120B 미디움과 대등한 성능을 보였다. 업스테이지는 20조 규모의 고품질 사전학습 데이터셋을 구축해 성능을 끌어올렸다고 설명했다. 업스테이지는 향후 일부 데이터셋을 한국지능정보사회진흥원(NIA)의 AI 허브에 공개해 국내 AI 연구 생태계 활성화를 지원할 계획이다. 솔라오픈은 129개 전문가 모델을 혼합한 혼합전문가 MoE 구조를 채택했다. MoE는 여러 전문가 모델을 묶어 두고 입력마다 필요한 일부만 선택해 활성화함으로써 전체 계산량을 줄이면서 성능을 높이는 구조다.김성훈 업스테이지 대표는 &quot;솔라 오픈은 업스테이지가 독자적으로 학습한 모델&quot;이라며 &quot;한국의 정서와 언어적 맥락을 깊이 이해하는 가장 한국적이면서도 세계적인 AI&quot;라고 말했다

---

*Crawled at: 2026-01-06 04:02:01*
*Source: [원문 보기](https://n.news.naver.com/mnews/article/293/0000077380)*
